{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exploratory Data Analysis and Deep Learning on Indian Housing Data\n#### In this notebook I am going to be taking a look at data on rentla properties in 6 major cities in India in 2022. The data was taken from the following link: https://www.kaggle.com/datasets/iamsouravbanerjee/house-rent-prediction-dataset","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Getting the data\n\nDataSet = pd.read_csv('../input/house-rent-prediction-dataset/House_Rent_Dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2022-08-16T01:39:55.657847Z","iopub.execute_input":"2022-08-16T01:39:55.658791Z","iopub.status.idle":"2022-08-16T01:39:55.724394Z","shell.execute_reply.started":"2022-08-16T01:39:55.658623Z","shell.execute_reply":"2022-08-16T01:39:55.723321Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Let's visualize the data by looking at the rent distribution in each city. I chose to plot the Log of the rent so that the large values dont dominate","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nDataSet['LogRent'] = np.log10(DataSet['Rent'])\nDataSet['LogSize'] = np.log10(DataSet['Size'])\nsns.violinplot(data = DataSet,x='City',y='LogRent')\nplt.title('Violin Plot of Rent ')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T01:40:55.331429Z","iopub.execute_input":"2022-08-16T01:40:55.331962Z","iopub.status.idle":"2022-08-16T01:40:56.384808Z","shell.execute_reply.started":"2022-08-16T01:40:55.331922Z","shell.execute_reply":"2022-08-16T01:40:56.383478Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"#### From the violin plot, we can see how each city has a unique distribution. For example, we can see that the rental market in Bangalore is comprised mostly of lower rent homes, but has some properties going for vastly higher rents. In general, the cheapest city to live in is Kolkata and the most expensive city is Mumbai which is more or less in line with what we would expect","metadata":{}},{"cell_type":"markdown","source":"### Let's make another visualization to see how floor space relates to rent prices","metadata":{}},{"cell_type":"code","source":"sns.jointplot(data=DataSet,x='LogSize',y='LogRent',hue='City')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T01:40:56.454196Z","iopub.execute_input":"2022-08-16T01:40:56.454675Z","iopub.status.idle":"2022-08-16T01:40:57.428900Z","shell.execute_reply.started":"2022-08-16T01:40:56.454639Z","shell.execute_reply":"2022-08-16T01:40:57.427685Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"#### From this we can see that Delhi has a bimodal distribution, so we will take a closer look","metadata":{}},{"cell_type":"code","source":"g = sns.jointplot(data=DataSet[DataSet['City'] == 'Delhi'],x='LogSize',y='LogRent',kind='hist')\ng.plot_joint(sns.kdeplot, color=\"r\", zorder=1, levels=4)\ng.fig.suptitle('Housing Data in Mumbai',size='xx-large',va='bottom')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T01:40:57.641024Z","iopub.execute_input":"2022-08-16T01:40:57.642327Z","iopub.status.idle":"2022-08-16T01:40:58.658346Z","shell.execute_reply.started":"2022-08-16T01:40:57.642279Z","shell.execute_reply":"2022-08-16T01:40:58.657025Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"#### Just as we thought, there seems to be two different trends going on here. Let's see if we can use unsupervised learning to separate the two sets of data","metadata":{}},{"cell_type":"markdown","source":"## Hierarchical Clustering\n### We can use hierarchical clustering to create a dendrogram showing how our data points are related","metadata":{}},{"cell_type":"code","source":"from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\nimport matplotlib.pyplot as plt\n\n\nlink_mat = linkage(DataSet[DataSet['City'] == 'Delhi'][['LogRent','LogSize']], 'ward') # This creates the linkage, aka the hierarchy\n\nplt.title('Hierarchical Clustering Dendrogram') # Plotting the dendrogram \nplt.axis('off') # Removing the axes because labels arent important and they overlap\ndendrogram(link_mat,truncate_mode='lastp',p=200) # Alter p to show a truncated diagram\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T01:40:59.442055Z","iopub.execute_input":"2022-08-16T01:40:59.442508Z","iopub.status.idle":"2022-08-16T01:41:02.002363Z","shell.execute_reply.started":"2022-08-16T01:40:59.442472Z","shell.execute_reply":"2022-08-16T01:41:02.000887Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"#### From the dendrogram, we can see that there is strong evidence for either 2 or 4 different natural clusters in the Delhi data","metadata":{}},{"cell_type":"markdown","source":"### Let's now plot our data again using two clusters to see if we have successfully separated the two trends that existed in our data","metadata":{}},{"cell_type":"code","source":"n_clusters = 4 # Number of classes to get labels for\nh_label = fcluster(link_mat,t=n_clusters,criterion='maxclust') # Getting the class labels\n\nfor i in range(len(h_label)):\n    if h_label[i] != 4:\n        h_label[i] = 0\n    else:\n        h_label[i] = 1\n\ng = sns.jointplot(data=DataSet[DataSet['City'] == 'Delhi'],x='LogSize',y='LogRent',hue=h_label)\ng.fig.suptitle('Housing Data in Mumbai',size='xx-large',va='bottom')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T01:41:02.004238Z","iopub.execute_input":"2022-08-16T01:41:02.005070Z","iopub.status.idle":"2022-08-16T01:41:02.641676Z","shell.execute_reply.started":"2022-08-16T01:41:02.005017Z","shell.execute_reply":"2022-08-16T01:41:02.640334Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"#### We were indeed successful, and now instead of having one bimodal distribution we have separated our data into two unimodal distributions","metadata":{}},{"cell_type":"markdown","source":"# Machine Learning for Rent Prediction\n### Now that we have looked at our data, let's see if we can use deep learning in order to predict rental prices given a description of the property. The first step toward doing this is feature engineering/data cleaning.\n\n#### The first thing I would like to do is convert the floor data from being nominal data containing strings to two sets of ordinal data. The way that the 'Floor' data is stored as a string saying 'floor the apartment is on' out of 'number of floors in the building'. Often, the floor is indicated by the word 'Ground' or 'Upper'. I will have to deal with these cases as well. Finally, I will add a column in our dataframe called 'Height' which gives the ration between the floor the apartment is on and the number of floors the building has.","metadata":{}},{"cell_type":"code","source":"floor = []; numfloor = [];\nfor element in DataSet['Floor']:\n    floor.append(element.split()[0])\n    numfloor.append(element.split()[-1])\n\nDataSet['NumberFloors'] = numfloor\nDataSet['Level'] = floor\nDataSet['NumberFloors'] = DataSet['NumberFloors'].replace(to_replace='Ground',value=0)\nDataSet['Level'] = DataSet['Level'].replace(to_replace='Ground',value=0)\nDataSet['Level'] = DataSet['Level'].replace(to_replace='Lower',value=0)\nDataSet.loc[DataSet['Level'] == 'Upper', 'Level'] = DataSet['NumberFloors']\nDataSet['Level'] = pd.to_numeric(DataSet['Level']) + 1\nDataSet['NumberFloors'] = pd.to_numeric(DataSet['NumberFloors']) + 1\nDataSet['Height'] = DataSet['Level']/DataSet['NumberFloors']","metadata":{"execution":{"iopub.status.busy":"2022-08-16T01:41:02.884085Z","iopub.execute_input":"2022-08-16T01:41:02.884523Z","iopub.status.idle":"2022-08-16T01:41:02.915945Z","shell.execute_reply.started":"2022-08-16T01:41:02.884481Z","shell.execute_reply":"2022-08-16T01:41:02.912804Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Time to build our model. We will create a feedforward deep neural network with batch normalization to assist in training. In order to deal with all of the categorical data the we have, we will one-hot encode our data. Finally, we will separate our data into train, test, and validation sets and then scale our data.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nprint(\"num GPUs available:\", len(tf.config.experimental.list_physical_devices(\"GPU\")))\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization, Activation\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\n\n# Initializing scalers\n\nscaler = RobustScaler()\nscaler2 = StandardScaler()\n\n# Performing one-hot encoding\n\nDataSet2 = pd.get_dummies(DataSet, columns=['Area Type','Area Locality','City','Furnishing Status','Tenant Preferred'])\n\n# Removing outliers\n\nq_high = DataSet2['Rent'].quantile(0.99)\nq_low = DataSet2['Rent'].quantile(0.01)\nDataSet2 = DataSet2[(DataSet2['Rent'] < q_high) & (DataSet2['Rent'] > q_low)]\n\n# Forming train and test sets\n\nX = DataSet2.drop(columns = ['Rent','Floor','LogRent','LogSize','Point of Contact','Posted On'])\ny = DataSet2['Rent']\n\nx_train, x_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)\ny_train_df = pd.DataFrame(data=y_train)\n\n# Scaling the data\n\nx_train_scaled = scaler.fit_transform(x_train)\ny_train_scaled = scaler2.fit_transform(y_train_df)\n\nx_train, x_val, y_train, y_val = train_test_split(x_train_scaled,y_train_scaled, test_size = 0.2)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-16T01:41:04.573858Z","iopub.execute_input":"2022-08-16T01:41:04.574306Z","iopub.status.idle":"2022-08-16T01:41:12.210950Z","shell.execute_reply.started":"2022-08-16T01:41:04.574272Z","shell.execute_reply":"2022-08-16T01:41:12.209652Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Building the model\n\nMSE_model = Sequential()\nMSE_model.add(Dense(256, activation = 'relu', input_dim = len(X.columns)))\nMSE_model.add(Dense(128))\nMSE_model.add(BatchNormalization())\nMSE_model.add(Activation('relu'))\nMSE_model.add(Dense(128))\nMSE_model.add(BatchNormalization())\nMSE_model.add(Activation('relu'))\nMSE_model.add(Dense(128))\nMSE_model.add(BatchNormalization())\nMSE_model.add(Activation('relu'))\nMSE_model.add(Dense(1, activation = 'linear'))\nMSE_model.compile(optimizer='adam',loss='mean_squared_error',metrics=['mean_absolute_percentage_error'])\nMSE_model.summary()\n\n# Getting the history to plot the training\n\nhistory = MSE_model.fit(x_train_scaled, y_train_scaled, epochs=100,validation_data=(x_val,y_val), verbose = False)\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Training History')\nplt.xlabel('Epoch')\nplt.ylabel('MSE')\nplt.legend(['Training','Validation'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T01:41:12.213076Z","iopub.execute_input":"2022-08-16T01:41:12.213812Z","iopub.status.idle":"2022-08-16T01:42:36.339843Z","shell.execute_reply.started":"2022-08-16T01:41:12.213769Z","shell.execute_reply":"2022-08-16T01:42:36.338577Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"#### As we can see, the training stabilizes at around 100 epochs, and we don't see a divergence between the training and validation data","metadata":{}},{"cell_type":"code","source":"pred = scaler2.inverse_transform(MSE_model.predict(scaler.transform(x_test)))\nreal = y_test\npred_df = pd.DataFrame(data=pred)\nreal_df = pd.DataFrame(data=real)\nreal_df.reset_index(inplace=True)\nreal_df.drop(columns = ['index'],inplace = True)\nprint('The Mean Absolute Percentage Error is:',sum(abs(real_df['Rent'] - pred_df[0]))/real_df['Rent'].sum())","metadata":{"execution":{"iopub.status.busy":"2022-08-16T01:42:36.341207Z","iopub.execute_input":"2022-08-16T01:42:36.341556Z","iopub.status.idle":"2022-08-16T01:42:36.699157Z","shell.execute_reply.started":"2022-08-16T01:42:36.341525Z","shell.execute_reply":"2022-08-16T01:42:36.698064Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### We know the Mean Absolute Percentage Error of our model, but it is important to visualize or results to make sure we are aware of the drawbacks of our model","metadata":{}},{"cell_type":"code","source":"ax = sns.histplot((real_df['Rent'] - pred_df[0])/real_df['Rent']*100,kde=True)\nax.lines[0].set_color('crimson')\nplt.title('Histogram of Rent Predictions')\nplt.xlabel('Mean Percentage Error (%)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T01:42:36.701716Z","iopub.execute_input":"2022-08-16T01:42:36.702201Z","iopub.status.idle":"2022-08-16T01:42:37.075835Z","shell.execute_reply.started":"2022-08-16T01:42:36.702167Z","shell.execute_reply":"2022-08-16T01:42:37.074398Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"#### As we can see, most of our predictions are on the mark, but our distribution is left-skewed. This means that sometimes we are predicting that rents will be much higher than they actually are. This is probably due to the fact that in each city, a small amount of apartments will be far more expensive than the rest. We trained our network using MSE which will heavily punish these errors. Therefore our network will approach a weighted geometric mean between these high rents and the more common low rent values, giving us a left-skewed distribution","metadata":{}},{"cell_type":"markdown","source":"### In order to mitigate the issue with over-estimating rents, I will do more feature engineering. Perhaps we can capture the cases when rent is much higher than expected by clustering our data and adding the cluster labels as a feature\n\n#### Last time we used hierarchical clustering, but this time we will do k-means.","metadata":{}},{"cell_type":"code","source":"import sklearn.cluster as cluster\n\nkmeans = cluster.KMeans(n_clusters = 18, init = 'k-means++')\nkmeans = kmeans.fit(DataSet[['Rent','Size','BHK','Level','NumberFloors']])\n\nDataSet['Cluster'] = kmeans.labels_","metadata":{"execution":{"iopub.status.busy":"2022-08-16T01:42:37.077451Z","iopub.execute_input":"2022-08-16T01:42:37.077831Z","iopub.status.idle":"2022-08-16T01:42:38.534467Z","shell.execute_reply.started":"2022-08-16T01:42:37.077796Z","shell.execute_reply":"2022-08-16T01:42:38.533456Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nprint(\"num GPUs available:\", len(tf.config.experimental.list_physical_devices(\"GPU\")))\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization, Activation\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\n\n# Initializing scalers\n\nscaler = RobustScaler()\nscaler2 = StandardScaler()\n\n# Performing one-hot encoding\n\nDataSet2 = pd.get_dummies(DataSet, columns=['Area Type','Area Locality','City','Furnishing Status','Tenant Preferred'])\n\n# Removing outliers\n\nq_high = DataSet2['Rent'].quantile(0.99)\nq_low = DataSet2['Rent'].quantile(0.01)\nDataSet2 = DataSet2[(DataSet2['Rent'] < q_high) & (DataSet2['Rent'] > q_low)]\n\n# Forming train and test sets\n\nX = DataSet2.drop(columns = ['Rent','Floor','LogRent','LogSize','Point of Contact','Posted On'])\ny = DataSet2['Rent']\n\nx_train, x_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)\ny_train_df = pd.DataFrame(data=y_train)\n\n# Scaling the data\n\nx_train_scaled = scaler.fit_transform(x_train)\ny_train_scaled = scaler2.fit_transform(y_train_df)\n\nx_train, x_val, y_train, y_val = train_test_split(x_train_scaled,y_train_scaled, test_size = 0.2)\n\n# Building the model\n\nMSE_model2 = Sequential()\nMSE_model2.add(Dense(256, activation = 'relu', input_dim = len(X.columns)))\nMSE_model2.add(Dense(128))\nMSE_model2.add(BatchNormalization())\nMSE_model2.add(Activation('relu'))\nMSE_model2.add(Dense(128))\nMSE_model2.add(BatchNormalization())\nMSE_model2.add(Activation('relu'))\nMSE_model2.add(Dense(128))\nMSE_model2.add(BatchNormalization())\nMSE_model2.add(Activation('relu'))\nMSE_model2.add(Dense(1, activation = 'linear'))\nMSE_model2.compile(optimizer='adam',loss='mean_squared_error',metrics=['mean_absolute_percentage_error'])\nMSE_model2.summary()\n\n# Getting the history to plot the training\n\nhistory = MSE_model2.fit(x_train_scaled, y_train_scaled, epochs=100,validation_data=(x_val,y_val), verbose = False)\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Training History')\nplt.xlabel('Epoch')\nplt.ylabel('MSE')\nplt.legend(['Training','Validation'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T01:42:38.536170Z","iopub.execute_input":"2022-08-16T01:42:38.537333Z","iopub.status.idle":"2022-08-16T01:44:02.895191Z","shell.execute_reply.started":"2022-08-16T01:42:38.537283Z","shell.execute_reply":"2022-08-16T01:44:02.893928Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"pred2 = scaler2.inverse_transform(MSE_model2.predict(scaler.transform(x_test)))\nreal2 = y_test\npred_df2 = pd.DataFrame(data=pred2)\nreal_df2 = pd.DataFrame(data=real2)\nreal_df2.reset_index(inplace=True)\nreal_df2.drop(columns = ['index'],inplace = True)\nprint('The Mean Absolute Percentage Error When Trained with Cluster Labels is:',sum(abs(real_df2['Rent'] - pred_df2[0]))/real_df2['Rent'].sum())\n\ndf1 = pd.DataFrame(data=(real_df['Rent'] - pred_df[0])/real_df['Rent']*100)\ndf2 =  pd.DataFrame((real_df2['Rent'] - pred_df2[0])/real_df2['Rent']*100)\ndf1['label'] = np.zeros(len(real_df))\ndf2['label'] = np.ones(len(real_df2))\nframes = [df1, df2]\ndf = pd.concat(frames,ignore_index=True)\ndf['label'].replace(0,'Original Model', inplace = True)\ndf['label'].replace(1,'Trained on Clustered Data', inplace = True)\n\nsns.displot(data=df,x=0,hue='label',kind='kde')\nplt.title('Histogram of Rent Predictions')\nplt.xlabel('Mean Percentage Error (%)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T01:44:02.897561Z","iopub.execute_input":"2022-08-16T01:44:02.898535Z","iopub.status.idle":"2022-08-16T01:44:03.759322Z","shell.execute_reply.started":"2022-08-16T01:44:02.898472Z","shell.execute_reply":"2022-08-16T01:44:03.757737Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### As we hoped, the clustering did in fact help. We have a much stronger central peak and fewer points in the over-estimate region","metadata":{}},{"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVR\nfrom sklearn import metrics\n\nscaler3 = StandardScaler()\n\nx_train_svr, x_test_svr, y_train_svr, y_test_svr = train_test_split(X,y, test_size = 0.2)\ny_train_svr_df = pd.DataFrame(data=y_train_svr)\ny_test_svr_df = pd.DataFrame(data=y_test_svr)\ny_train_svr_scaled = scaler3.fit_transform(y_train_svr_df)\ny_test_svr_scaled = scaler3.transform(y_test_svr_df)\n\nmy_pipe = Pipeline([('scaler', StandardScaler()), ('regressor',SVR())])\n\nparams = [{'scaler'                   : [MinMaxScaler(), StandardScaler(), RobustScaler()],\n           'regressor__kernel'       : ['poly', 'rbf'],\n           'regressor__degree'       : [1,2,3,4],\n           'regressor__C'            : [0.5, 0.8, 1.0, 2.0]}]\n\ngrid = GridSearchCV(my_pipe, params, cv=5, scoring = 'neg_root_mean_squared_error')\ngrid.fit(x_train_svr, y_train_svr_scaled.ravel())\n\n# Reporting best parameters\n\nprint(grid.best_params_)\nprint(grid.score(x_test, y_test.ravel()))","metadata":{"execution":{"iopub.status.busy":"2022-08-16T04:18:53.505393Z","iopub.execute_input":"2022-08-16T04:18:53.505821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svr = SVR(degree=2,kernel='poly',C=1)\nsvr.fit(x_train, y_train)\nsvr.score(x_val,y_val)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T04:07:29.221410Z","iopub.execute_input":"2022-08-16T04:07:29.221841Z","iopub.status.idle":"2022-08-16T04:07:38.118288Z","shell.execute_reply.started":"2022-08-16T04:07:29.221806Z","shell.execute_reply":"2022-08-16T04:07:38.117303Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"pred3 = scaler2.inverse_transform((svr.predict(scaler.transform(x_test))).reshape(-1, 1))\nreal3 = y_test\npred_df3 = pd.DataFrame(data=pred3)\nreal_df3 = pd.DataFrame(data=real3)\nreal_df3.reset_index(inplace=True)\nreal_df3.drop(columns = ['index'],inplace = True)\nprint('The Mean Absolute Percentage Error When Trained with Cluster Labels and SVR is:',sum(abs(real_df3['Rent'] - pred_df3[0]))/real_df3['Rent'].sum())\n\n\ndf1 = pd.DataFrame(data=(real_df['Rent'] - pred_df[0])/real_df['Rent']*100)\ndf2 =  pd.DataFrame((real_df2['Rent'] - pred_df2[0])/real_df2['Rent']*100)\ndf3 =  pd.DataFrame((real_df3['Rent'] - pred_df3[0])/real_df3['Rent']*100)\ndf1['label'] = np.zeros(len(real_df))\ndf2['label'] = np.ones(len(real_df2))\ndf3['label'] = 2*np.ones(len(real_df3))\nframes = [df1, df2, df3]\ndf = pd.concat(frames,ignore_index=True)\ndf['label'].replace(0,'Original Model', inplace = True)\ndf['label'].replace(1,'Trained on Clustered Data', inplace = True)\ndf['label'].replace(2,'SVR Trained on Clustered Data', inplace = True)\n\nsns.displot(data=df,x=0,hue='label',kind='kde')\nplt.title('Histogram of Rent Predictions')\nplt.xlabel('Mean Percentage Error (%)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T04:07:52.708633Z","iopub.execute_input":"2022-08-16T04:07:52.709086Z","iopub.status.idle":"2022-08-16T04:07:54.982206Z","shell.execute_reply.started":"2022-08-16T04:07:52.709048Z","shell.execute_reply":"2022-08-16T04:07:54.981041Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"### Can we do more feature engineering?\n\n#### When we did our one-hot encoding, the area localities feature had over 2000 unique entries. This is a lot of features and we end up with a quite sparse matrix. Instead of naively one-hot encoding the area localities, we can instead try to group the localities into k clusters and one-hot encode based on the k labels\n","metadata":{}},{"cell_type":"markdown","source":"### To cluster based on Area Locality, I first make a pivot table to aggregate the data for each locality","metadata":{}},{"cell_type":"code","source":"DataSet.drop(columns = ['Cluster'],inplace=True)\ndata_pivot = pd.pivot_table(data=DataSet[['BHK','Rent','Size','Bathroom','Level','NumberFloors','Area Locality']],index='Area Locality')\ndata_pivot","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:10:27.505473Z","iopub.execute_input":"2022-08-12T08:10:27.505926Z","iopub.status.idle":"2022-08-12T08:10:27.542748Z","shell.execute_reply.started":"2022-08-12T08:10:27.505886Z","shell.execute_reply":"2022-08-12T08:10:27.541048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We then perform the clustering. I chose to use 60 clusters, perhaps corresponding to 10 for each city","metadata":{}},{"cell_type":"code","source":"scaler3 = RobustScaler()\ndata_pivot_scaled = scaler3.fit_transform(data_pivot)\nkmeans2 = cluster.KMeans(n_clusters = 60, init = 'k-means++')\nkmeans2 = kmeans2.fit(data_pivot_scaled)\n\ndata_pivot['Cluster'] = kmeans2.labels_\n\nsns.countplot(data=data_pivot,x='Cluster',order = data_pivot['Cluster'].value_counts().index)\nplt.xticks([])\nplt.plot()","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:02:50.737164Z","iopub.execute_input":"2022-08-12T08:02:50.737546Z","iopub.status.idle":"2022-08-12T08:02:52.303441Z","shell.execute_reply.started":"2022-08-12T08:02:50.737515Z","shell.execute_reply":"2022-08-12T08:02:52.302428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### I plotted the counts for each cluster to ensure not all localities were clustered to only a few centroids","metadata":{}},{"cell_type":"code","source":"DataSet_clust = DataSet.drop(columns = ['Posted On','LogSize','LogRent','Height','Floor'])\nfor i in range(len(data_pivot.index)):\n    DataSet_clust['Area Locality'] = DataSet_clust['Area Locality'].replace(to_replace=data_pivot.index[i],value=data_pivot['Cluster'][i])\n    \n\nscaler = RobustScaler()\nscaler2 = StandardScaler()\n\n# Performing one-hot encoding\n\nDataSet3 = pd.get_dummies(DataSet_clust, columns=['Area Type','Area Locality','City','Furnishing Status','Tenant Preferred'])\n\n# Removing outliers\n\nq_high = DataSet3['Rent'].quantile(0.99)\nq_low = DataSet3['Rent'].quantile(0.01)\nDataSet3 = DataSet3[(DataSet3['Rent'] < q_high) & (DataSet3['Rent'] > q_low)]\n\n# Forming train and test sets\n\nX = DataSet3.drop(columns = ['Rent','Point of Contact'])\ny = DataSet2['Rent']\n\nx_train, x_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)\ny_train_df = pd.DataFrame(data=y_train)\n\n# Scaling the data\n\nx_train_scaled = scaler.fit_transform(x_train)\ny_train_scaled = scaler2.fit_transform(y_train_df)\n\nx_train, x_val, y_train, y_val = train_test_split(x_train_scaled,y_train_scaled, test_size = 0.2)\n\n# Building the model\n\nMSE_model3 = Sequential()\nMSE_model3.add(Dense(256, activation = 'relu', input_dim = len(X.columns)))\nMSE_model3.add(Dense(128))\nMSE_model3.add(BatchNormalization())\nMSE_model3.add(Activation('relu'))\nMSE_model3.add(Dense(128))\nMSE_model3.add(BatchNormalization())\nMSE_model3.add(Activation('relu'))\nMSE_model3.add(Dense(128))\nMSE_model3.add(BatchNormalization())\nMSE_model3.add(Activation('relu'))\nMSE_model3.add(Dense(1, activation = 'linear'))\nMSE_model3.compile(optimizer='adam',loss='mean_squared_error',metrics=['mean_absolute_percentage_error'])\nMSE_model3.summary()\n\n# Getting the history to plot the training\n\nhistory = MSE_model3.fit(x_train_scaled, y_train_scaled, epochs=100,validation_data=(x_val,y_val), verbose = False)\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Training History')\nplt.xlabel('Epoch')\nplt.ylabel('MSE')\nplt.legend(['Training','Validation'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:03:00.925457Z","iopub.execute_input":"2022-08-12T08:03:00.926438Z","iopub.status.idle":"2022-08-12T08:03:49.481895Z","shell.execute_reply.started":"2022-08-12T08:03:00.926400Z","shell.execute_reply":"2022-08-12T08:03:49.480937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### As with previous models, our training stabilizes around 100 epochs with no major diversion between train and validation.","metadata":{}},{"cell_type":"code","source":"pred3 = scaler2.inverse_transform(MSE_model3.predict(scaler.transform(x_test)))\nreal3 = y_test\npred_df3 = pd.DataFrame(data=pred3)\nreal_df3 = pd.DataFrame(data=real3)\nreal_df3.reset_index(inplace=True)\nreal_df3.drop(columns = ['index'],inplace = True)\nprint('The Mean Absolute Percentage Error is:',sum(abs(real_df3['Rent'] - pred_df3[0]))/real_df3['Rent'].sum())","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:03:49.483867Z","iopub.execute_input":"2022-08-12T08:03:49.484539Z","iopub.status.idle":"2022-08-12T08:03:49.724146Z","shell.execute_reply.started":"2022-08-12T08:03:49.484497Z","shell.execute_reply":"2022-08-12T08:03:49.723026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finally, we will compare the original model with the naive one-hot encoding, the model with one-hot encoding as well as an added feature of labels from k-means clustering, and the new model we just made","metadata":{}},{"cell_type":"code","source":"df1 = pd.DataFrame(data=(real_df['Rent'] - pred_df[0])/real_df['Rent']*100)\ndf2 =  pd.DataFrame((real_df2['Rent'] - pred_df2[0])/real_df2['Rent']*100)\ndf3 =  pd.DataFrame((real_df3['Rent'] - pred_df3[0])/real_df3['Rent']*100)\ndf1['label'] = np.zeros(len(real_df))\ndf2['label'] = np.ones(len(real_df2))\ndf3['label'] = np.ones(len(real_df2))*2\nframes = [df1, df2, df3]\ndf = pd.concat(frames,ignore_index=True)\ndf['label'].replace(0,'Original Model', inplace = True)\ndf['label'].replace(1,'Trained on Clustered Data', inplace = True)\ndf['label'].replace(2,'Trained with Clustered Localities', inplace = True)\n\nsns.displot(data=df,x=0,hue='label',kind='kde')\nplt.title('Histogram of Rent Predictions')\nplt.xlabel('Mean Percentage Error (%)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-12T08:03:49.995775Z","iopub.execute_input":"2022-08-12T08:03:49.996692Z","iopub.status.idle":"2022-08-12T08:03:50.438339Z","shell.execute_reply.started":"2022-08-12T08:03:49.996655Z","shell.execute_reply":"2022-08-12T08:03:50.437364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### As you can see, our new model underperforms compared to the other models. This is not surprising given the fact that we have far less features to learn on as compared to the other models. However, if computation time is a larger factor, perhaps the reduction in accuracy is worth it here","metadata":{}}]}